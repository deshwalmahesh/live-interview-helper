{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"fastapi[standard]\" nest-asyncio pyngrok uvicorn openai transformers -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Must DO:\n",
    "1. Go and Signup at [`ngrok`](https://dashboard.ngrok.com/signup) (One time thing)\n",
    "2. After Successful login go to `https://dashboard.ngrok.com/get-started/setup/linux` and you'll see something like the below code line. Run it with you token\n",
    "3. Just run every line as it is\n",
    "4. Once your app will run it'll show random url like  `Public URL:https://2127-35-186-148-235.ngrok-free.app` (it'll change everytime you run the code)\n",
    "5. Paste the above URL in your local `config.json -> transcription -> API_ENDPOINT`. If it is empty there, your local will model in your system\n",
    "6. Run your local app as usual given in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKQqd2-nqN8v"
   },
   "outputs": [],
   "source": [
    "\n",
    "!ngrok config add-authtoken <YOUR_NGROK_TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8kcfGEappqL"
   },
   "source": [
    "# Load Whisper and Expose `/transcribe` Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSO45Mn4m5VP"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import asyncio, re, torch\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from typing import List\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import logging\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import base64\n",
    "from typing import Optional\n",
    "import asyncio\n",
    "from transformers import pipeline\n",
    "import logging\n",
    "\n",
    "\n",
    "# Whisper settings\n",
    "WHISPER_LANGUAGE = \"english\"\n",
    "TRANSCRIPTION_MODEL_NAME = \"openai/whisper-large-v3-turbo\"\n",
    "MAX_SENTENCE_CHARACTERS = 128\n",
    "\n",
    "# Diarization settings\n",
    "NUM_SPEAKERS = 2\n",
    "\n",
    "device_name = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "device = torch.device(device_name)\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "# ------ Transcription Helpers ------\n",
    "transcription_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    TRANSCRIPTION_MODEL_NAME, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ")\n",
    "transcription_model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(TRANSCRIPTION_MODEL_NAME)\n",
    "\n",
    "transcription_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=transcription_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s = 30, #  min(LENGTH_IN_SEC, 30)\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware, allow_origins=['*'], allow_credentials=True, allow_methods=['*'], allow_headers=['*'])\n",
    "\n",
    "class AudioRequest(BaseModel):\n",
    "    audio_data: str  # Base64 encoded audio data\n",
    "\n",
    "class TranscriptionResponse(BaseModel):\n",
    "    text: str\n",
    "    timestamps: Optional[list] = None\n",
    "    error: Optional[str] = None\n",
    "\n",
    "async def process_transcription(audio_array: np.ndarray, sample_rate: int = 16000, language: str = \"english\", return_timestamps: bool = False):\n",
    "    \"\"\"\n",
    "    Process the audio array through the transcription pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return await asyncio.to_thread(\n",
    "            transcription_pipeline,\n",
    "            {\"array\": audio_array, \"sampling_rate\": sample_rate},\n",
    "            return_timestamps=return_timestamps,\n",
    "            generate_kwargs={\n",
    "                \"language\": language,\n",
    "                \"return_timestamps\": return_timestamps,\n",
    "                \"max_new_tokens\": MAX_SENTENCE_CHARACTERS\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Transcription processing error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Transcription processing failed: {str(e)}\")\n",
    "\n",
    "\n",
    "@app.post(\"/transcribe\", response_model=TranscriptionResponse)\n",
    "async def transcribe_audio(request: AudioRequest):\n",
    "    \"\"\"\n",
    "    Endpoint to receive audio data and return transcription.\n",
    "\n",
    "    Expects base64 encoded audio data in the request body.\n",
    "    Returns transcription text and optional timestamps.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Decode base64 audio data\n",
    "        try:\n",
    "            audio_bytes = base64.b64decode(request.audio_data)\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=400, detail=\"Invalid base64 audio data\")\n",
    "\n",
    "        # Convert to numpy array (assuming 16-bit PCM audio)\n",
    "        try:\n",
    "            audio_array = np.frombuffer(audio_bytes, np.int16).astype(np.float32) / 32768.0\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=400, detail=\"Failed to process audio data\")\n",
    "\n",
    "        # Validate audio data\n",
    "        if len(audio_array) == 0:\n",
    "            raise HTTPException(status_code=400, detail=\"Empty audio data\")\n",
    "\n",
    "        # Process transcription\n",
    "        result = await process_transcription(audio_array)\n",
    "\n",
    "        return TranscriptionResponse(text=result[\"text\"])\n",
    "\n",
    "    except HTTPException as he:\n",
    "        raise he\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in transcribe_audio: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")\n",
    "\n",
    "# Optional: Add a health check endpoint\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc7QLandplZ4"
   },
   "source": [
    "# Run Code\n",
    "\n",
    "\n",
    "Copy the `Public URL` in your local of `/config`. Everytime you run the below code, it'll be a different URL so need to change in the config file too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1az_Ec8zfXO"
   },
   "outputs": [],
   "source": [
    "import torch, nest_asyncio, uvicorn\n",
    "from pyngrok import ngrok\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "N8kcfGEappqL"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
